https://blog.jrwang.me/2019/flink-source-code-data-exchange/

Flink 的数据交换机制在设计时遵循两个基本原则：
 1. 数据交换的控制流（例如，为初始化数据交换而发出的消息）是由接收端发起的
 2. 数据交换的数据流（例如，在网络中实际传输的数据被抽象为 IntermediateResult 的概念）是可插拔的。
在一个 TaskManager 中可能会同时并行运行多个Task，每个 Task 都在单独的线程中运行。
在不同的 TaskManager 中运行的 Task 之间进行数据传输要基于网络进行通信
通信是基于 Netty 创建的标准的 TCP 连接，同一个 TaskManager 内运行的不同 Task 会复用网络连接。

一，从节点角色的角度看shuffle
假设有两个 TaskManager，每个 TaskManager 都分别运行一个 map Task 和一个 reduce Task。
首先，M1 产出了一个 ResultPartition(RP1)。当这个 RP 可以被消费的时候，TaskManager会通知JobManager。
JobManager 会通知将要拉取RP分区数据的接收者（tasks R1 and R2）当前分区数据已经准备好。
如果下游的数据接收方还没有被调度，将会触发对应任务的部署. 接受方会从 RP 中请求数据。
数据传输可能是本地的，也可能是通过 TaskManager 的网络栈进行。


二，从节点内部角度看
上游生成的数据通过 Collector 收集，并传递给 RecordWriter 对象。RecordWriter 包含一组序列化器，每个下游的数据消费者 Task 分别对应一个。
ChannelSelector 会选择一个或多个序列化器处理记录。
    如果记录需要被广播，那么就会被交给每一个序列化器进行处理
    如果记录是按照 hash 进行分区的，ChannelSelector 会计算记录的哈希值，然后选择对应的序列化器。

序列化器会将记录序列化为二进制数据，并将其存放在固定大小的 buffer 中。
buffer 被交给 BufferWriter 处理，写入到 ResulePartition（RP）中。
RP有多个子分区构成，每一个子分区都只收集特定消费者需要的数据。
由于第一个 Buffer 已经生成，RS2 就变成可被消费的状态了（注意，这个行为实现了一个 streaming shuffle），通知 JobManager。

JobManager查找RS2的消费者，通知 TaskManager 2 一个数据块已经可以访问。
通知TM2的消息会被发送到InputChannel，该inputchannel被认为是接收这个buffer的，接着通知RS2可以初始化一个网络传输。
RS2通过TM1的网络栈请求该buffer，双方基于 Netty 准备进行数据传输。网络连接是在TaskManager（而非特定的task）之间长时间存在的，
而不是在独立的 Task 之间。
一旦 Buffer 被 TM2 接收，进入 InputGate（它包含多个IC），进入一个反序列化器（RecordDeserializer），从 buffer 中将记录还原成指定类型的对象，
然后将其传递给接收数据的 Task。

三，源码
3.1 熟悉相关概念
Flink 作业运行时产生的中间结果的抽象。

IntermediateDataset
JobGraph 中对中间结果的抽象。
把可以 chain 到一起 operator 合并为一个 JobVertex，IntermediateDataset 就表示一个 JobVertex 的输出结果。
JobVertex 的输入是 JobEdge，JobEdge 可以看作是 IntermediateDataset 的消费者。
一个 JobVertex 也可能产生多个 IntermediateDataset, 一个 IntermediateDataset 只会有一个 JobEdge 作为消费者，一个
JobVertex 的下游有多少 JobVertex 需要依赖当前节点的数据，当前节点就有对应数量的 IntermediateDataset。

IntermediateResult 和 IntermediateResultpartition
在 JobManager 中，JobGraph 转换成可以被调度的并行化版本的执行图，即 ExecutionGraph。
和 JobVertex 对应的节点是 ExecutionJobVertex，和 IntermediateDataset 对应的则是 IntermediataResult。
由于一个节点可能有多个并行子任务运行，ExecutionJobVertex 按并行度的设置被拆分为多个 ExecutionVertex，每一个表示一个并行的子任务。
同样的，一个 IntermediataResult 也被拆分为多个 IntermediateResultPartition， 对应 ExecutionVertex 的输出结果。

ResultPartition 和 ResultSubpartition
表示并行子任务的 ExecutionVertex 会被调度到 TaskManager 中执行，一个 Task 对应一个 ExecutionVertex。
输出结果 IntermediateResultPartition 相对应的则是 ResultPartition。IntermediateResultPartition
可能会有多个 ExecutionEdge 作为消费者，那么在 Task 这里，ResultPartition 就会被拆分为多个 ResultSubpartition，
下游每一个需要从当前 ResultPartition 消费数据的 Task 都会有一个专属的 ResultSubpartition。

ResultPartitionType 指定了 ResultPartition 的不同属性，包括是否流水线模式、是否会产生反压以及是否限制使用的 Network buffer 的数量。
ResultPartitionType 有三个枚举值：
    BLOCKING：非流水线模式，无反压，不限制使用的网络缓冲的数量
    PIPELINED：流水线模式，有反压，不限制使用的网络缓冲的数量
    PIPELINED_BOUNDED：流水线模式，有反压，限制使用的网络缓冲的数量

其中是否流水线模式这个属性会对消费行为产生很大的影响：
    流水线模式，在 ResultPartition 接收到第一个 Buffer 时，消费者任务就可以进行准备消费。
    非流水线模式，消费者将等到生产端任务生产完数据之后才进行消费。

InputGate 和 InputChannel
在 Task 中，InputGate 是对输入的封装，InputGate 是和 JobGraph 中 JobEdge 一一对应的。
每个 InputGate 消费了一个或多个 ResultPartition。
InputGate 由 InputChannel 构成，InputChannel 和 ExecutionGraph 中的 ExecutionEdge 一一对应；
InputChannel 和 ResultSubpartition 一一相连，一个 InputChannel 接收一个 ResultSubpartition 的输出。
根据读取的 ResultSubpartition 的位置，InputChannel 有 LocalInputChannel 和 RemoteInputChannel 两种不同的实现。

3.2 shuffle源码细节
一个典型的生产者-消费者模型，上游算子生产数据到 ResultPartition 中，下游算子通过 InputGate 消费数据。
由于不同的 Task 可能在同一个 TaskManager 中运行，也可能在不同的 TaskManager 中运行：
通过合理的设计和抽象，Flink 确保本地数据交换和通过网络进行数据交换可以复用同一套代码。

Task 的输入和输出
输出：
Task 的每一个 ResultPartition 都对应一个 ResultPartitionWriter，也有一个 LocalBufferPool 负责写入数据所需的 buffer。
Task 启动会向 NetworkEnvironment 注册，为每一个 ResultPartition 分配 LocalBufferPool，申请的是MemeorySegment。
ResultPartion 实现 ResultPartitionWriter 接口：
成员有：
	// ResultPartition 由 ResultSubpartition 构成，
	// ResultSubpartition 的数量由下游消费 Task 数和 DistributionPattern 来决定。
	// 例如，如果是 FORWARD，则下游只有一个消费者；如果是 SHUFFLE，则下游消费者的数量和下游算子的并行度一样
	private final ResultSubpartition[] subpartitions;

	//ResultPartitionManager 管理当前 TaskManager 所有的 ResultPartition
	private final ResultPartitionManager partitionManager;

	//通知当前ResultPartition有数据可供消费的回调函数回调
	private final ResultPartitionConsumableNotifier partitionConsumableNotifier;

Task 通过 RecordWriter 将结果写入 ResultPartition 中。
RecordWriter 是对 ResultPartitionWriter 的一层封装，负责将记录对象序列化到 buffer 中。
内部维护的成员如下：
  //决定一条记录应该写入哪一个channel， 即 sub-partition
	private final ChannelSelector<T> channelSelector;
	//channel的数量，即 sub-partition的数量
	private final int numberOfChannels;
	//序列化
	private final RecordSerializer<T> serializer;

当 Task 通过 RecordWriter 输出一条记录时，主要流程为：
通过 ChannelSelector 确定写入的目标 channel
使用 RecordSerializer 对记录进行序列化
向 ResultPartition 请求 BufferBuilder，用于写入序列化结果
向 ResultPartition 添加 BufferConsumer，用于读取写入 Buffer 的数据

输入：
Task 通过循环调用 InputGate.getNextBufferOrEvent 方法获取输入数据。
SingleInputGate：内部维护的一个队列，有数据时就加入到队列中，在需要获取数据时从队列中取出一个 channel，获取
channel 中的数据。
InputChannel 有 LocalInputChannel 和 RemoteInputChannel 两中不同的实现，分别对应本地和远程数据交换：
LocalInputChannel：同一个 JVM 进程内不同线程之间进行，无需通过网络交换。
不同 Task 之间的网络传输基于 Netty 实现。
NetworkEnvironment 中通过 ConnectionManager 来管理所有的网络的连接，而 NettyConnectionManager 就是 ConnectionManager 的具体实现
NettyConnectionManager 在启动的时候会创建并启动 NettyClient 和 NettyServer，NettyServer 会启动一个服务端监听，等待其它 NettyClient 的连接：
当 RemoteInputChannel 请求一个远端的 ResultSubpartition ，NettyClient 请求的 ResultSubpartition 所在 Task 的 NettyServer 的连接，
后续数据交换都在这个连接上进行。两个 Task 之间只会建立一个连接，这个连接会在不同的 RemoteInputChannel 和 ResultSubpartition 之间进行复用。
