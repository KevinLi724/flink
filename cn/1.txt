一，原因：
1. 实际生产环境，面对各种情况下未知流量的波动。
2. 计算平台的用户，不清楚如何配置作业资源，根据监控上报反压情况，一次次调整作业的并行度。
二，现有解决方案
1. 实时收集作业运行的metric，比如反压情况，cpu/内存利用率，吞吐量等指标，最终定位发生瓶颈的任务节点，做出响应的扩缩容策略。
先紧急触发一次checkpoint，保存当前任务的快照和中间状态，停止作业调整作业的并行度，重新启动作业。
  痛点：
      1. 利用外部指标，每次只能推断出单个Task节点，并没有利用上DAG任务中的上下游依赖关系，调整到最优状态需要多次。
      2. 对于多租户场景，同一个物理机上的其他Task会彼此影响，导致扩缩容的策略不准确。
      3.真正执行扩缩容动作需要触发一次checkpoint，然后重新启动作业，持续时间过长，导致上游数据积压。
三，实现需求
1. 不停止作业实现自适应扩缩容。
2. 用户不用设置作业并行度，作业启动后自动收敛到最优状态。
3. 考虑整个DAG任务的上下游数据依赖，一次计算出最优态。
四，拆解
4.1. 基于算法实现最优扩缩容执行计划的生成。
  统计每个operator算子的真实处理和输出能力，依据DAG拓扑结构图，每个算子扩缩容后如何影响下游的operator。
  这里定义了基于处理数据速率的计算模型，每个拉取数据的时间窗口内：
    真实处理速度：窗口时间内从上游拉取的数据条数/窗口时间内用于数据处理的时间（一条数据序列化，处理，到反序列化）
    观测处理速度：窗口时间内从上游拉取的数据条数/一个Task端到端的处理时间
    真实输出速度：窗口时间内从下游push的数据条数/窗口时间内用于数据处理的时间
    观测输出速度：窗口时间内从下游push的数据条数/一个Task端到端的处理时间
    由上述计算出每个operator的生产消费比。
   系统实现：
        1. 每处理一条数据就上报是不现实的，因此在每个worker实现类MetricManager，周期行的收集当前处理的速率
        2. Master拿到一个Metric的客户端，固定时间从各个worker上拉取指定的速率数据，提供聚合计算的功能。
        3. 所有上报来的数据将推送到扩缩容决策器中，基于上述迭代计算的算法，计算出最优的并行度。将新的执行计划给到Master
        在线调度新的执行计划。
4.2. 基于步骤1，worker节点并行调整最优执行计划。
    每个worker实现的是并行调整。
    Flink 通过keygroup的概念（）
